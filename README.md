# Methodology

### Step 1 — Synthetic problem generation

Synthesize new, complex problems by using problems from different fields as seeds.
In this example, 1,000 synthetic problems were generated using mathematical and physics problems as seeds.

- Generate drafts for cross-domain problems by using problems from different fields as seeds.
- Apply methods such as [evol-instruct](https://arxiv.org/abs/2306.08568) and [WizardMath](https://arxiv.org/abs/2308.09583) to refine drafts.
- Introduce quantitative indicators to assess problem quality and use them as criteria for revision. Problems with low computational cost and high intrinsic difficulty are considered high-quality. By iteratively revising and scoring problems, we produce higher-quality synthetic problems.

### Step 2 — Answer generation

We performed 10 rollouts per question to estimate answers to the synthetic problems, collecting reasoning tokens and final answers.  
(Currently, rollouts have been completed only for IDs 1–200 out of the 1,000 synthetic problems.  [dataset](https://huggingface.co/datasets/tarona/MathXPhys_scored_v1))

### Step 3 — Converting LaTeX-formatted answers to SymPy format

To quantitatively assess the consistency of the final answers obtained through rollouts, we converted answers from LaTeX to SymPy format.

### Step 4 — Verification of self-consistency

Based on self-consistency scores, we selected the most appropriate final answer for each question.

## Details of each step

Details of the [dataset](https://huggingface.co/datasets/tarona/MathXPhys_scored_v1) generated by this methodology:

| Step | Implementation details                          | LLM              | Model                   | Script                      | HF subset                                                  |
|------|------------------------------------------------|------------------|-------------------------|-----------------------------|------------------------------------------------------------|
| 1    | Synthetic problem generation                   | API (OpenRouter) | "deepseek-r1-0528:free" | problem_gen_manager.py      | OB_PHYS_problem                                            |
| 2    | Answer generation with rollouts                | API (OpenRouter) | "deepseek-r1-0528:free" | rollout_manager.py          | OB_PHYS_rollout                                            |
| 3    | Converting LaTeX-formatted answers to SymPy    | Local (vllm)     | "Qwen3-8b-AWQ"          | sympy_conversion_manager.py | OB_PHYS_rollout_sympy                                      |
| 4    | Verification of self-consistency               | ----             | ----                    | sympy_self_consistency_manager.py | OB_PHYS_self_consistency, OB_PHYS_self_consistency_rollout |

## Seed data used for cross-domain problem generation

Two distinct fields were used as seed data to generate interdisciplinary problems. In this example, the seed data were mathematical and physics problems.

### Mathematical seed data
We used 1,177 problems from the [OlympiadBench (MIT license)](https://github.com/OpenBMB/OlympiadBench) dataset in text-only format ('OE_TO_maths_en_COMP', 'TP_TO_maths_en_COMP') as mathematical seed data. This dataset contains four subfields: "Combinatorics", "Algebra", "Number Theory", and "Geometry".

### Physics seed data
We used 999 problems from the [PHYSICS: A Comprehensive Benchmark for Advanced Physics Reasoning (MIT license)](https://github.com/yale-nlp/Physics) dataset in text-only format ('PHYSICS-textonly') as physics seed data. This dataset contains six subfields: "atomic", "electro", "mechanics", "optics", "quantum", and "statistics".


## Usage

Create a `.env` file and enter your API key as shown below (OpenRouter recommended):

```
OPENROUTER_API_KEY=your_api_key_here
```

When pushing to Hugging Face, also set the following token:
```
HF_TOKEN=your_token_here
```

## Conclusion
We have generated synthetic data intended for training large language models (LLMs). However, due to our limited experience of training LLMs and the lack of an execution environment, we have been unable to verify the dataset's validity. We would appreciate it if someone could evaluate the dataset and share the results.
